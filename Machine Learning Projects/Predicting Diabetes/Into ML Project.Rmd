---
title: "Intro ML Project on Diabetes"
author: "Benny Frisella"
date: "2025-5-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(root.dir = "C:/Users/benny/Documents/University of Texas @ Dallas/Year 3/Intro to Machine Learning/Datasets")
```

Exploratory Data Analysis.
```{r}
# Exploratory Analysis

diabetes <- read.csv("C:/Users/benny/Documents/University of Texas @ Dallas/Year 3/Intro to Machine Learning/Datasets/diabetes.csv")


# summary(diabetes) -> medium-sized set
colnames(diabetes)

#find missing values (they were set to zero) but it's impossible for these to be zero
sum(diabetes$Glucose == 0)        # 13
sum(diabetes$BloodPressure == 0)  # 90
sum(diabetes$SkinThickness == 0)  # 573
sum(diabetes$BMI == 0)        # 28
sum(diabetes$Age == 0)    # 0
sum(diabetes$Insulin == 0)     # 956
sum(diabetes$DiabetesPedigreeFunction == 0)   # 0

which(diabetes$Pregnancies == max(diabetes$Pregnancies))     # 3 results
```
```{r}
#perform LDA, using MASS to save space
library(MASS)

#fit LDA model using all variables to predict
LDA <- lda(Outcome ~ ., data = diabetes)

#predict
predictions <- predict(LDA, diabetes)

#check
#predictions$class

#create confusion matrix (shorthand)
table(Predicted = predictions$class, Actual = diabetes$Outcome)

#classify based on a 0.5 cutoff
predictedClasses <- ifelse(predictions$posterior >= 0.5, 1, 0)

```

```{r}
library(pROC)

LDA.ROC <- roc(diabetes$Outcome, predictions$posterior[, 2])

#plot Reciever Operating Characteristic
plot(LDA.ROC, col = "blue", main = "LDA ROC Curve")

#print Area Under Curve
AUC.LDA <- auc(LDA.ROC)
AUC.LDA

text(0.4, 0.2, paste("AUC =", round(AUC.LDA, 4)), col = "blue", cex = 1.2)

```
```{r}
#fit QDA model using all variables to predict
QDA <- qda(Outcome ~ ., data = diabetes)

#predict
predictions2 <- predict(QDA, diabetes)

#check
#predictions2$class

#create confusion matrix (shorthand)
table(Predicted = predictions2$class, Actual = diabetes$Outcome)

#classify based on a 0.5 cutoff
predictedClasses2 <- ifelse(predictions2$posterior >= 0.5, 1, 0)
```

```{r}
QDA.ROC <- roc(diabetes$Outcome, predictions2$posterior[, 2])

#plot Receiver Operating Characteristic
plot(QDA.ROC, col = "red", main = "QDA ROC Curve")

#print Area Under Curve
AUC.QDA <- auc(QDA.ROC)
AUC.QDA

text(0.4, 0.2, paste("AUC QDA =", round(AUC.QDA, 4)), col = "red", cex = 1.2)

```

```{r}
#test other posterior probabilities for QDA and LDA
predictedClasses <- ifelse(predictions$posterior[, 2] >= 0.33, 1, 0)

# Classify based on a 0.5 cutoff (or other cutoff like 0.33)
predictedClasses2 <- ifelse(predictions2$posterior[, 2] >= 0.25, 1, 0)

#create confusion matrix (shorthand)
confusionQDA <- table(Predicted = predictedClasses2, Actual = diabetes$Outcome)

#create confusion matrix (shorthand)
confusionLDA <- table(Predicted = predictedClasses, Actual = diabetes$Outcome)

```

```{r}
#calculate metrics
confusionMetrics <- function(confusionMatrix) 
{
  TP <- confusionMatrix[2, 2]  #true Positives
  TN <- confusionMatrix[1, 1]  #true Negatives
  FP <- confusionMatrix[1, 2]  #false Positives
  FN <- confusionMatrix[2, 1]  #false Negatives
  
  accuracy <- (TP + TN) / sum(confusionMatrix)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  f1_score <- 2 * (precision * recall) / (precision + recall)

  return(c(Accuracy = accuracy, Precision = precision, Recall = recall, Specificity = specificity, F1_Score = f1_score))
}

#QDA
metrics_QDA <- confusionMetrics(confusionQDA)
print("Metrics for QDA:")
print(metrics_QDA)

#LDA
metrics_LDA <- confusionMetrics(confusionLDA)
print("Metrics for LDA:")
print(metrics_LDA)
```

retrace to EDA
```{r}

library(VIM)

prop.table(table(diabetes$Outcome))
# 65.8 % are not diagnosed, while 34.2% are

#remove insulin predictor
diabetes <- subset(diabetes, select = -Insulin..)

diabetes$Glucose..[diabetes$Glucose.. == 0] <- mean(diabetes$Glucose..[diabetes$Glucose.. != 0])
diabetes$BMI..[diabetes$BMI.. == 0] <- mean(diabetes$BMI..[diabetes$BMI.. != 0])

sum(diabetes$Glucose.. == 0)        # now zero
sum(diabetes$BMI.. == 0)        # now zero

#remove outcome
df_features <- diabetes[, !names(diabetes) %in% "Outcome"]

#colnames(df_features)

# standardize numeric features (z-score: (x - mean) / sd)
df_scaled <- as.data.frame(scale(df_features))

# use for KNN, without response variable outcome
df_scaled_copy <- df_scaled

df_imputed <- kNN(df_scaled_copy, variable = "BloodPressure..", k = 5)
df_imputed <- kNN(df_scaled_copy, variable = "SkinThickness..", k = 5)

df_imputed$Outcome <- diabetes$Outcome

df_imputed <- subset(df_imputed, select = -SkinThickness.._imp)

sum(df_imputed$BloodPressure.. == 0)  # zero now
sum(df_imputed$SkinThickness.. == 0)  # zero now

```

Check for multicollinearity
```{r}
library(reshape2)
#extract just the predictor variables
predictors <- df_imputed[, !(names(df_imputed) %in% "Outcome")]

#compute correlation matrix
cor_matrix <- cor(predictors)

#round for readability
cor_matrix_rounded <- round(cor_matrix, 2)

# Convert to long format (flatten the matrix)
cor_long <- melt(cor_matrix)

# Remove self-correlations (Var1 == Var2)
cor_long <- cor_long[cor_long$Var1 != cor_long$Var2, ]

# Remove duplicate pairs like (A,B) and (B,A)
cor_long_unique <- cor_long[!duplicated(t(apply(cor_long[, 1:2], 1, sort))), ]

# Sort by absolute correlation
cor_all_sorted <- cor_long_unique[order(abs(cor_long_unique$value), decreasing = TRUE), ]

# Print the summary
print("All pairwise correlations (sorted by strength):")
print(cor_all_sorted)
```

Run LDA/QDA again
```{r}
###LDA
lda_model <- lda(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. +
                 SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age..,
                 data = trainData)

lda_pred <- predict(lda_model, testData)
lda_class <- lda_pred$class

#confusion
table(lda_class, testData$Outcome)

#accuracy
mean(lda_class == testData$Outcome)

###QDA
qda_model <- qda(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. +
                 SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age..,
                 data = trainData)


qda_pred <- predict(qda_model, testData)
qda_class <- qda_pred$class

#confusion
table(qda_class, testData$Outcome)

#accuracy
mean(qda_class == testData$Outcome)


```
```{r}
lda_TN <- 362
lda_FP <- 38
lda_FN <- 89
lda_TP <- 111

qda_TN <- 346
qda_FP <- 54
qda_FN <- 86
qda_TP <- 114

#create comparison table
summary_table <- data.frame(
  Model = c("LDA", "QDA"),
  Accuracy = c(0.788, 0.767),
  Sensitivity = c(lda_TP / (lda_TP + lda_FN), qda_TP / (qda_TP + qda_FN)),
  Specificity = c(lda_TN / (lda_TN + lda_FP), qda_TN / (qda_TN + qda_FP)),
  TP = c(lda_TP, qda_TP),
  TN = c(lda_TN, qda_TN),
  FP = c(lda_FP, qda_FP),
  FN = c(lda_FN, qda_FN)
)

# Print the table
print(summary_table)
```
Fit a logistic regression model using all predictors except Insulin.
```{r}


logReg <- glm(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. + SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age.., family = binomial, data = df_imputed)

#Randomly split data into: 70% training and 30% prediction sets
#set.seed(8)
sampleSize <- floor(0.7 * nrow(df_imputed))
trainIndex <- sample(seq(nrow(df_imputed)), size = sampleSize)

#include randomly chosen indices for training
trainData <- df_imputed[trainIndex,]

#include what wasn't chosen in training set into test set
testData <- df_imputed[-trainIndex,]



#Build model on training set
fit <- glm(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. 
           + SkinThickness.. + BMI.. 
           + DiabetesPedigreeFunction.. + Age.., 
           family = binomial, data = trainData)

summary(fit)

#estimated probabilities for test data
logReg.prob <- predict(fit, testData, type = "response")

#predicted classes (using 0.5 cutoff)
logReg.pred <- ifelse(logReg.prob >= 0.5, 1, 0)

#test error rate
1 - mean(logReg.pred == testData[, "Outcome"])



# Confusion matrix and (sensitivity, specificity)
# `+' = 1, `-' = 0

table(logReg.pred, testData[, "Outcome"])

TN <- 336
FP <- 49
FN <- 84
TP <- 131

errorRate <- (FP + FN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)  # recall
specificity <- TN / (TN + FP)

cat("\nError rate: (FP + FN) / (TP + TN + FP + FN) =", errorRate)
cat("\nSensitivity: TP / (TP + FN) =", sensitivity)
cat("\nSpecificity: TN / (TN + FP) =", specificity)

```

```{r}
library(glmnet)
#add regularization
### ridge if alpha = 0, lasso for alpha = 1


# Set up predictors and outcome
x <- model.matrix(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. + 
                  SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age.., 
                  data = df_imputed)[, -1]  # Remove intercept column

y <- df_imputed$Outcome

# Run Ridge regression (alpha = 0)
logReg_ridge <- cv.glmnet(x, y, family = "binomial", alpha = 0)

# Plot cross-validation curve
plot(logReg_ridge)

# Coefficients at best lambda
print(coef(logReg_ridge, s = "lambda.min")

#recreate
x_test <- model.matrix(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. +
                       SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age..,
                       data = testData)[, -1]

ridge_probs <- predict(logReg_ridge, newx = x_test, s = "lambda.min", type = "response")
ridge_pred <- ifelse(ridge_probs >= 0.5, 1, 0)

#evaluate
confusion <- table(Predicted = ridge_pred, Actual = testData$Outcome)
accuracy <- mean(ridge_pred == testData$Outcome)
print(confusion)
cat("\nAccuracy:", accuracy)

```
```{r}
#re-use for lasso
### ridge if alpha = 0, lasso for alpha = 1


# Set up predictors and outcome
x <- model.matrix(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. + 
                  SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age.., 
                  data = df_imputed)[, -1]  #remove intercept

y <- df_imputed$Outcome

#run(alpha = 1)
logReg_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 2)

# Plot cross-validation curve
plot(logReg_lasso)

# Coefficients at best lambda
coef(logReg_lasso, s = "lambda.min")

#recreate
x_test2 <- model.matrix(Outcome ~ Pregnancies.. + Glucose.. + BloodPressure.. +
                       SkinThickness.. + BMI.. + DiabetesPedigreeFunction.. + Age..,
                       data = testData)[, -1]

lasso_probs <- predict(logReg_lasso, newx = x_test2, s = "lambda.min", type = "response")
lasso_pred <- ifelse(lasso_probs >= 0.5, 1, 0)

#evaluate
confusion <- table(Predicted = lasso_pred, Actual = testData$Outcome)
accuracy <- mean(lasso_pred == testData$Outcome)
print(confusion)
cat("\nAccuracy:", accuracy)

```
```{r}
#analyze feature selection from lasso
# Get all coefficients at the best lambda
lasso_coef <- coef(logReg_lasso, s = "lambda.min")

# Convert to a tidy data frame (optional, for clarity)
library(tibble)
lasso_df <- as.data.frame(as.matrix(lasso_coef))
lasso_df <- rownames_to_column(lasso_df, var = "Predictor")
colnames(lasso_df)[2] <- "Coefficient"

# View only predictors not shrunk to zero
lasso_df[lasso_df$Coefficient != 0, ]
```
```{r}
library(pROC)

# Example for Lasso
lasso_roc <- roc(testData$Outcome, as.vector(lasso_probs))
plot(lasso_roc, col = "red", main = "ROC Curves")

# Add other models
ridge_roc <- roc(testData$Outcome, as.vector(ridge_probs))
lines(ridge_roc, col = "blue")

lda_roc <- roc(testData$Outcome, lda_pred$posterior[,2])
lines(lda_roc, col = "green")

qda_roc <- roc(testData$Outcome, qda_pred$posterior[,2])
lines(qda_roc, col = "black")

legend("bottomright", legend = c("Lasso", "Ridge", "LDA", "QDA"),
       col = c("red", "blue", "green", "black"), lwd = 2)

```


back to EDA
```{r}
#pair matrix
library(GGally)

ggpairs(diabetes[, !(names(diabetes) %in% "Outcome")])
```

```{r}
#check for a funnel
fitted_vals <- fitted(logReg)
residuals_vals <- resid(logReg, type = "deviance")

# Create the plot
plot(fitted_vals, residuals_vals,
     xlab = "Fitted Values",
     ylab = "Deviance Residuals",
     main = "Residuals vs Fitted Values",
     pch = 20, col = "blue")
abline(h = 0, lty = 2, col = "red")
```

add Insulin back
```{r}
library(VIM)
library(GGally)

#adjust EDA again
diabetes <- read.csv("C:/Users/benny/Documents/University of Texas @ Dallas/Year 3/Intro to Machine Learning/Datasets/diabetes.csv")

diabetes_clean <- subset(diabetes, Insulin.. != 0)

#replace 0s in Glucose and BMI with column means
diabetes_clean$Glucose..[diabetes_clean$Glucose.. == 0] <-
  mean(diabetes_clean$Glucose..[diabetes_clean$Glucose.. != 0])

diabetes_clean$BMI..[diabetes_clean$BMI.. == 0] <-
  mean(diabetes_clean$BMI..[diabetes_clean$BMI.. != 0])

sum(diabetes_clean$Glucose.. == 0)        # check
sum(diabetes_clean$BMI.. == 0)        # good

#scale first
clean_scaled <- as.data.frame(scale(diabetes_clean[, !(names(diabetes_clean) %in% "Outcome")]))
clean_scaled$Outcome <- diabetes_clean$Outcome

# KNN impute selected variables
new_df_imputed <- kNN(clean_scaled, variable = c("BloodPressure..", "SkinThickness.."), k = 5)
df_final <- new_df_imputed[, !grepl("_imp$", names(new_df_imputed))]

#check correlations
ggpairs(df_final[, !(names(df_final) %in% "Outcome")])
```

```{r}
###using data subset from now on...

# Define numeric_vars (exclude Outcome)
numeric_vars <- df_final[, !(names(df_final) %in% "Outcome")]

#compute correlation matrix
cor_matrix <- round(cor(numeric_vars), 3)
cor_df <- as.data.frame(as.table(cor_matrix))

#remove self-correlations
cor_df <- cor_df[cor_df$Var1 != cor_df$Var2, ]

#add absolute value for sorting
cor_df$abs_cor <- abs(cor_df$Freq)

#remove duplicate pairs (e.g., A,B and B,A)
cor_df_unique <- cor_df[!duplicated(t(apply(cor_df[,1:2], 1, sort))), ]

#sort and view top correlations
top_corrs <- cor_df_unique[order(-cor_df_unique$abs_cor), ]

#only top 10
head(top_corrs, 10)
```

Random Forest
```{r}
library(randomForest)

# Set random seed for reproducibility
set.seed(42)

# Split data into train/test (70/30)
sample_size <- floor(0.7 * nrow(df_final))  # assuming df_final is your cleaned dataset
train_indices <- sample(seq_len(nrow(df_final)), size = sample_size)

train_data <- df_final[train_indices, ]
test_data <- df_final[-train_indices, ]

# Fit Random Forest model
rf_model <- randomForest(as.factor(Outcome) ~ ., data = train_data, importance = TRUE)

# Predict on test data
rf_pred <- predict(rf_model, newdata = test_data)

# Confusion matrix
confusion <- table(Predicted = rf_pred, Actual = test_data$Outcome)
print(confusion)

# Accuracy
accuracy <- mean(rf_pred == test_data$Outcome)
cat("Random Forest Accuracy:", accuracy, "\n")

# Variable importance
importance(rf_model)
varImpPlot(rf_model)


```
```{r}
#Random Forest confusion matrix
TP <- 84  # True Positives: predicted 1, actual 1
TN <- 214 # True Negatives: predicted 0, actual 0
FP <- 11  # False Positives: predicted 1, actual 0
FN <- 5  # False Negatives: predicted 0, actual 1

# Confusion Matrix variables
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
errorRate <- (FP + FN) / (TP + TN + FP + FN)

cat("Random Forest Statistics:\n\n")
cat("Sensitivity:", round(sensitivity, 3), "\n")
cat("Specificity:", round(specificity, 3), "\n")
cat("Error Rate:", round(errorRate, 3), "\n")
cat("Accuracy:", round(1-errorRate, 3), "\n")

# Precision and Recall
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

# F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("F1 Score:", round(f1_score, 3), "\n")
```

```{r}

library(xgboost)

# 1. Prepare data
set.seed(42)
sample_index <- sample(seq_len(nrow(df_final)), size = 0.7 * nrow(df_final))

train_data <- df_final[sample_index, ]
test_data <- df_final[-sample_index, ]

# Separate features and labels
train_matrix <- as.matrix(train_data[, !names(train_data) %in% "Outcome"])
train_label <- train_data$Outcome

test_matrix <- as.matrix(test_data[, !names(test_data) %in% "Outcome"])
test_label <- test_data$Outcome

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)

xgb_model <- xgboost(data = dtrain,
                     objective = "binary:logistic",
                     nrounds = 50,
                     max_depth = 3,
                     eta = 0.1,
                     verbose = 0)

# Predict probabilities
xgb_probs <- predict(xgb_model, newdata = dtest)

# Classify using 0.5 threshold
xgb_pred <- ifelse(xgb_probs >= 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(Predicted = xgb_pred, Actual = test_label)
print(conf_matrix)

# Accuracy
accuracy <- mean(xgb_pred == test_label)
cat("XGBoost Accuracy:", accuracy, "\n")


```

XGBoost
```{r}
# XGBoost confusion matrix
TP <- 71  # True Positives: predicted 1, actual 1
TN <- 200 # True Negatives: predicted 0, actual 0
FP <- 25  # False Positives: predicted 1, actual 0
FN <- 18  # False Negatives: predicted 0, actual 1

# Confusion Matrix variables
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
errorRate <- (FP + FN) / (TP + TN + FP + FN)

cat("XGBoost Statistics:\n\n")
cat("Sensitivity:", round(sensitivity, 3), "\n")
cat("Specificity:", round(specificity, 3), "\n")
cat("Error Rate:", round(errorRate, 3), "\n")
cat("Accuracy:", round(1-errorRate, 3), "\n")

# Precision and Recall
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

# F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("F1 Score:", round(f1_score, 3), "\n")

```

XGBoost Tune via Grid search
```{r}
library(caret)
library(xgboost)
library(dplyr)

df_final$Outcome <- as.factor(df_final$Outcome)
set.seed(42)
trainIndex <- createDataPartition(df_final$Outcome, p = 0.7, list = FALSE)
trainData <- df_final[trainIndex, ]
testData <- df_final[-trainIndex, ]
trainData$Outcome <- factor(ifelse(trainData$Outcome == 1, "Yes", "No"))
testData$Outcome <- factor(ifelse(testData$Outcome == 1, "Yes", "No"))


#define grid, train control
xgb_grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

#cross-validation at k =5
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

#training
xgb_model <- train(
  Outcome ~ .,
  data = trainData,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = xgb_grid,
  metric = "ROC"
)

```
```{r}
#predict on test set
xgb_pred <- predict(xgb_model, newdata = testData)

#confusion matrix
confusionMatrix(xgb_pred, testData$Outcome)


```


